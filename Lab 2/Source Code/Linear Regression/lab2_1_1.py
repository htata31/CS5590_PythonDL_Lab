# -*- coding: utf-8 -*-
"""LAB2_1_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18iqfxcvLkar2GvFKovI1GcaHjfXZORsB
"""

import numpy
import pandas
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import pandas as pd
from keras.optimizers import SGD, Adam, Adamax
from sklearn.model_selection import train_test_split
from keras.callbacks import TensorBoard
from sklearn.preprocessing import LabelEncoder
from keras import metrics
import matplotlib.pyplot as plt

dataset=pd.read_csv('drive/My Drive/Python_ICP/insurance.csv')
le = LabelEncoder()
dataset['region'] = le.fit_transform(dataset['region'].astype('str'))
dataset['sex'] = le.fit_transform(dataset['sex'].astype('str'))
dataset['smoker'] = le.fit_transform(dataset['smoker'].astype('str'))

print(dataset.head())
# dataset = dataset.values
# split into input (X) and output (Y) variables
X = dataset.iloc[:,0:6]
Y = dataset.iloc[:,6]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
                                                    test_size=0.25, random_state=100)

# HyperParameters1
activation_function="tanh"
learning_rate=0.1
epochs=50
b_size=32
decay_rate= learning_rate / epochs
adam= Adam(lr=learning_rate, decay=decay_rate)

# HyperParameters2
activation_function="relu"
learning_rate=0.3
epochs=100
b_size=32
decay_rate= learning_rate / epochs
sgd= SGD(lr=learning_rate, decay=decay_rate)

#Define the model

model = Sequential()
model.add(Dense(50, input_dim = 6, activation=activation_function))
model.add(Dropout(0.1))
model.add(Dense(20, activation=activation_function))
model.add(Dense(10, activation=activation_function))
model.add(Dense(1,input_dim = 6, activation=activation_function))
  
model.compile(optimizer = "Adamax", loss = 'mean_squared_error', metrics = [metrics.mae])
tbCallBack = TensorBoard(log_dir='./Graph1', histogram_freq=0, write_graph=True, write_images=True)
hist = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epochs, batch_size=b_size,callbacks=[tbCallBack])

# Final evaluation of the model
mae, loss= model.evaluate(X_test, Y_test, verbose=0)
print(mae, loss)

# accuracy history
plt.plot(hist.history['mean_absolute_error'])
plt.plot(hist.history['val_mean_absolute_error'])
plt.title('model mae')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# loss
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# %load_ext tensorboard
# %tensorboard --logdir Graph1